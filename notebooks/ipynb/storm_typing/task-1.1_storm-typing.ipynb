{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "\n",
    "code_show=true; \n",
    "\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "    if (document.getElementById('input') !== null)\n",
    "        {document.getElementById('input').value = 'Display code in notebook';}\n",
    "} else {\n",
    "$('div.input').not(':first').show();\n",
    "document.getElementById('input').value = 'Hide code in notebook';\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "\n",
    "$( document ).ready(code_toggle);\n",
    "$('div.cell.code_cell.rendered.selected').find('div.input').hide();\n",
    "\n",
    "\n",
    "\n",
    "</script>\n",
    "\n",
    "<form id=\"form\" action=\"javascript:code_toggle()\">\n",
    "    <input style=\"position: fixed;top: 200;right: 0;\" id=\"input\" type=\"submit\" value=\"Display code in notebook\">\n",
    "</form>\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.pandas\n",
    "import sys\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from IPython.display import Markdown as md\n",
    "import cartopy.crs as ccrs\n",
    "import hvplot\n",
    "import holoviews as hv\n",
    "from geoviews import opts\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import s3fs\n",
    "import fsspec\n",
    "from bokeh.layouts import widgetbox\n",
    "from bokeh.layouts import column as bokehCol\n",
    "from bokeh.models.layouts import Column\n",
    "from bokeh.models.widgets import DatePicker\n",
    "from datetime import date\n",
    "import panel as pn\n",
    "import hvplot.xarray\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Local functions\n",
    "import sefm.backends.climate.storm_typing as st\n",
    "import sefm.backends.climate.zones as zn\n",
    "\n",
    "import sefm.utils.hydrotools as ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask client to implement distributed calculations (multicore or multinode if a cluster is available)\n",
    "client = Client(silence_logs='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storm typing classifies storms events into classes that are nearly homogeneous with respects to their generating phenomenon. This allows to evaluate more homogeunous distributions for each storm type when computing precipitation-frequency analysis rather than a traditionel mixed distribution with all the storms. Homogeneity is an assumption when computing frequency analysis which makes storm typing all the more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main storm types that can arise over the Outaouais watershed: \n",
    "- Mid-latitude cyclones (MLC),\n",
    "- Mesoscale storms (MEC)\n",
    "- Local storms (LS). \n",
    "\n",
    "These storm types have unique spatiotemporal characteristics which makes them nearly humogeneous for frequency analysis purposes. Specifically for the Outaouais watershed, the MLC and MLC/MEC hydrid storm types have the most impact as their duration is ofter longer than 2 days and because they usually cover a large area size which can lead to important flood volume in the watershed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Storm Type and Sub-Type                       | Acronym                 | Numerical Code |\n",
    "|-----------------------------------------------|-------------------------|----------------|\n",
    "| Mid-Latitude Cyclone                          | MLC                     | 10             |\n",
    "| Mid-Latitude Cyclone with Embedded Convection | MLC/EC                  | 13             |\n",
    "| Mesoscale Storm                               | MLC/EC                  | 30             |\n",
    "| Mesoscale Storm with Embedded Convection      | MLC/EC                  | 33             |\n",
    "| Local Storm                                   | MLC/EC                  | 40             |\n",
    "| Local Storm with Enhanced Convection          | LS/LEC                  | 43             |\n",
    "| MLC/MEC Hybrid                                |                         | 60             |\n",
    "| MLC/MEC Hybrid with Embedded Convection       |                         | 63             |\n",
    "| Dry Day                                       |                         | 99             |\n",
    "|                                               |                         |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database of Daily Storm Types (DDST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Storm typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two (2) domains are proposed to study precipitation in the Outaouais region and both of their pixels are aligned with the [20th century reanalysis V3's](https://psl.noaa.gov/data/gridded/data.20thC_ReanV3.html) pixels resolution:\n",
    "- Domain 1: A corser grid based on TVA's report. (4 zones)\n",
    "- Domain 2: A finer grid based on NB Power' report. (40 zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_definition = {'zone_1' : {'filenames':['grid-large.geojson', # polygon for coarse grids\n",
    "                                             'lines-large.geojson']}, # lines for coarse grids\n",
    "                    'zone_2' : {'filenames':['grid-small.geojson']}, # polygon for fine grids\n",
    "                    'background' : {'filenames': ['Bassins Outaouais HSAMI.geojson']}} # watershed in background\n",
    "\n",
    "zones = zn.Zones(zones=zones_definition,\n",
    "                 bucket='s3://sefm/gis',\n",
    "                 client_kwargs={\"endpoint_url\":\"https://s3.us-west-1.wasabisys.com\"})\n",
    "\n",
    "zones.interactive_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'zone_2_id'\n",
    "\n",
    "domain_dict = {'zone_1_id': 'Domain 1',\n",
    "               'zone_2_id': 'Domain 2'}\n",
    "\n",
    "md(\"\"\"Let's select %s to begin with...\"\"\"%(str(domain_dict[region])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Manual storm typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"\"\"For each zone inside the %s, the following variables can be calculated. \n",
    "Calculation for each data point is demonstrated in the following sub-sections :\"\"\"%(str(domain_dict[region])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Historical Climatology Network (GHCN)** :\n",
    "- cumulative precipitation (72h)\n",
    "\n",
    "**20th Century Reanalysis V3** :\n",
    "- precipitation rate;\n",
    "- cloud cover;\n",
    "- convective available potential energy;\n",
    "- precipitable water;\n",
    "- gradient in the 500-mb height;\n",
    "- gradient in the 850-mb height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use the **1979-09-14 - 1979-09-16** historical storm to vizualise all the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.1 Cumulative Precipitation (72h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Historical Climatology Network (GHCN) is an integrated database of climate summaries from land surface stations across the globe that have been subjected to a common suite of quality assurance reviews. We use this database to extract cumulated precipitation over a specified duration for each stations available over a specified region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on ghcnd's bucket\n",
    "bucket = 's3://ghcnd-can-us-ne'\n",
    "storage_options={'anon': True,\n",
    "                 \"client_kwargs\": {'endpoint_url': 'https://s3.us-east-2.wasabisys.com'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The data can be acquired from the following sources :\n",
    "- bucket : %s\n",
    "- endpoint_url : %s\n",
    "\"\"\"%(bucket, storage_options['client_kwargs']['endpoint_url'])\n",
    "\n",
    "md(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a bounding box to limit the metadata extraction to our study zone :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlngbox = [-82, -74, 44.5, 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"\"\"- Bounding box = %s  - *(lon_min, lon_max, lat_min, lat_max)*\"\"\"%(str(latlngbox)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stations available within our bounding box can be viewed in the following table : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = st.Stations(metadata_bucket=os.path.join(bucket,'ghcdn_stations.csv'),\n",
    "                       storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.read_metadata(latlngbox=latlngbox);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stations.read_parquet(os.path.join(bucket,'data/parquet/data.parquet'),\n",
    "                           element='PRCP',\n",
    "                           storage_options=storage_options)\n",
    "df['value'] = df['value']/10.0 # To convert into mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index='date', columns='id', values='value')\n",
    "df_pivot.index = pd.to_datetime(df_pivot.index)\n",
    "\n",
    "df_pivot = df_pivot[(df_pivot.index.year>=1900) & (df_pivot.index.year<2020)]\n",
    "df_pivot = df_pivot.rolling('3D').sum().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction for missing data\n",
    "stations.metadata = stations.metadata[~stations.metadata.id.isin(df_pivot.columns[df_pivot.count()==0].values)]\n",
    "\n",
    "stations.metadata = stations.metadata[~stations.metadata.id.isin(list(set(stations.metadata.id.values) - \n",
    "                                                                      set(df_pivot.columns)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations = gpd.GeoDataFrame(stations.metadata, \n",
    "                                geometry=gpd.points_from_xy(stations.metadata.longitude,\n",
    "                                                            stations.metadata.latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatiotemporal precipitation data availability can also be vizualised :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.plot_stations() + \\\n",
    "df_pivot.count(axis=1).resample('1Y').mean().hvplot(kind='scatter', grid=True,\n",
    "                                                   ylabel='Number of operating weather stations (rain)',\n",
    "                                                    width=600, height=400,\n",
    "                                                   title = 'Operating weather stations in the Outaouais region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddst = pd.DataFrame(columns = ['date','zone_id','variable','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.58\n",
    "\n",
    "zone_1_gdf = zones.data['zone_1']['gdf'][zones.data['zone_1']['gdf'].geometry.type.isin(['MultiPolygon'])].set_index('id')\n",
    "zone_2_gdf = zones.data['zone_2']['gdf'][zones.data['zone_2']['gdf'].geometry.type.isin(['Polygon'])].set_index('id')\n",
    "\n",
    "for index, zone_id in zone_1_gdf.iterrows():\n",
    "    stations.metadata.loc[stations.metadata.index.isin(gdf_stations[gdf_stations.within(zone_id.geometry)].index),\n",
    "                      'zone_1_id'] = index\n",
    "for index, zone_id in zone_2_gdf.iterrows():\n",
    "    stations.metadata.loc[stations.metadata.index.isin(gdf_stations[gdf_stations.within(zone_id.geometry)].index),\n",
    "                      'zone_2_id'] = index\n",
    "\n",
    "data = []\n",
    "\n",
    "for zone_id in sorted(stations.metadata[region].dropna().unique()):\n",
    "    df_pivot_zone = df_pivot.loc[:,stations.metadata[stations.metadata[region].isin([zone_id])].id]\n",
    "    \n",
    "    # Maximum precipitation\n",
    "    series = df_pivot_zone.max(axis=1)\n",
    "    dict_max_precip = {'date': series.index, \n",
    "                       'zone_id': int(zone_id),\n",
    "                       'variable': 'max_precip',\n",
    "                       'value': series.values}\n",
    "\n",
    "    data.extend([dict_max_precip])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_precip = zone_2_gdf.join(pd.concat([pd.DataFrame.from_dict(values) for values in data]).set_index('zone_id'))\n",
    "max_precip['longitude'] = max_precip.centroid.x\n",
    "max_precip['latitude'] = max_precip.centroid.y\n",
    "max_precip = max_precip.rename(columns={'date': 'time'})\n",
    "max_precip = max_precip.set_index(['time','longitude', 'latitude'])\n",
    "da_max_precip = xr.DataArray.from_series(max_precip['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremes storms events can therefore be displayed for the zones of Domain 2. For instance, the 1979-09-13 - 1979-09-15 72h total precipitation is shown below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_values = da_max_precip.sel(time='1979-09-15')\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da['value']]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 :72h total cumulative precipitation')*labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.2 Precipitation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 's3://cires-20-century-reanalysis-v3/zarr/single-levels'\n",
    "\n",
    "storage_options = {'endpoint_url': 'https://s3.us-east-1.wasabisys.com'}\n",
    "\n",
    "ds = xr.open_zarr(fsspec.get_mapper(bucket,\n",
    "                                    client_kwargs=storage_options,\n",
    "                                    anon=True),\n",
    "                  consolidated=True)\n",
    "\n",
    "ds_large_scale = ds\n",
    "\n",
    "ds = ds.sel(time=slice('1900-01-01', '2015-12-31'),\n",
    "            longitude=slice(latlngbox[0], latlngbox[1]),\n",
    "            latitude=slice(latlngbox[2], latlngbox[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['apcp72'] = da_max_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable='prate'\n",
    "\n",
    "da_values = ds[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "da_values = da_values*3600\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : maximum precipitation rate (72h window)')*labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.3 Cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable='tcdc'\n",
    "\n",
    "da_values = ds[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : maximum cloud cover (72h window)')*labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.4 Convective potential available energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable='cape'\n",
    "import geoviews.feature as gf\n",
    "# da_values = ds[variable].sel(time=slice('1979-09-14','1979-09-16')).max('time')\n",
    "da_values = ds[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "# data = list(zip(zone_plot.Polygons.values()[0].data.geometry.centroid.x,\n",
    "#                                         zone_plot.Polygons.values()[0].data.geometry.centroid.y))\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : maximum CAPE (72h window)')*labels\n",
    "# zones.data['background']['gdf'].hvplot(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hvplot = ds_large_scale[variable].sel(time=slice('1979-09-13','1979-09-15'))\\\n",
    "                        .max('time')\\\n",
    "# enlever le tooltip\n",
    "ds_hvplot.hvplot.contourf(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          hover=False,\n",
    "                          tiles='EsriUSATopo',\n",
    "                          alpha=0.2,\n",
    "                          cmap='rainbow')*\\\n",
    "ds_hvplot.hvplot.contour(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          alpha=1,\n",
    "                          cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.5 Precipitable water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable='pr_wtr'\n",
    "\n",
    "da_values = ds[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : maximum precipitable water (72h window)')*labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hvplot = ds_large_scale[variable].sel(time=slice('1979-09-13','1979-09-15'))\\\n",
    "                        .max('time')\\\n",
    "# enlever le tooltip\n",
    "ds_hvplot.hvplot.contourf(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          hover=False,\n",
    "                          tiles='EsriUSATopo',\n",
    "                          alpha=0.2,\n",
    "                          cmap='rainbow')*\\\n",
    "ds_hvplot.hvplot.contour(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          alpha=1,\n",
    "                          cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.6 500 mb Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 's3://cires-20-century-reanalysis-v3/zarr/pressure-levels'\n",
    "\n",
    "storage_options = {'endpoint_url': 'https://s3.us-east-1.wasabisys.com'}\n",
    "\n",
    "ds_pl = xr.open_zarr(fsspec.get_mapper(bucket,\n",
    "                                    client_kwargs=storage_options,\n",
    "                                    anon=True),\n",
    "                     consolidated=True)\n",
    "\n",
    "ds__pl_large_scale = ds_pl\n",
    "\n",
    "ds_pl = ds_pl.sel(time=slice('1900-01-01', '2015-12-31'),\n",
    "                  longitude=slice(latlngbox[0], latlngbox[1]),\n",
    "                  latitude=slice(latlngbox[2], latlngbox[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = np.sqrt(np.square(ds_pl.sel(level=500).hgt.differentiate('longitude')) + \n",
    "             np.square(ds_pl.sel(level=500).hgt.differentiate('latitude')))\n",
    "ds_pl['gradh500'] = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'gradh500'\n",
    "\n",
    "da_values = ds_pl[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : 500 mb maximum gradient (72h window)')*labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hvplot = ds__pl_large_scale['hgt'].sel(level=500, time=slice('1979-09-13','1979-09-15'))\\\n",
    "                                        .max('time')\\\n",
    "# enlever le tooltip\n",
    "ds_hvplot.hvplot.contourf(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          hover=False,\n",
    "                          tiles='EsriUSATopo',\n",
    "                          title='1979-09-15 : 500 mb maximum (72h window)',\n",
    "                          alpha=0.2,\n",
    "                          cmap='rainbow')*\\\n",
    "ds_hvplot.hvplot.contour(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          alpha=1,\n",
    "                          cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.6 900 mb Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = np.sqrt(np.square(ds_pl.sel(level=900).hgt.differentiate('longitude')) + \n",
    "             np.square(ds_pl.sel(level=900).hgt.differentiate('latitude')))\n",
    "ds_pl['gradh900'] = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'gradh900'\n",
    "\n",
    "da_values = ds_pl[variable].sel(time=slice('1979-09-13','1979-09-15')).max('time')\n",
    "\n",
    "df_da = da_values.to_dataframe().reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : 900 mb maximum gradient (72h window)')*labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hvplot = ds__pl_large_scale['hgt'].sel(level=900, time=slice('1979-09-13','1979-09-15'))\\\n",
    "                                        .max('time')\\\n",
    "# enlever le tooltip\n",
    "ds_hvplot.hvplot.contourf(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          hover=False,\n",
    "                          tiles='EsriUSATopo',\n",
    "                          title='1979-09-15 : 900 mb maximum (72h window)',\n",
    "                          alpha=0.2,\n",
    "                          cmap='rainbow')*\\\n",
    "ds_hvplot.hvplot.contour(levels=30,\n",
    "                          project=True,\n",
    "                          grid=True,\n",
    "                          alpha=1,\n",
    "                          cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3.8 Preliminary type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the rolling period\n",
    "ds_rolling_sl = ds[['cape','prate']].rolling(time=24).max().resample(time='1D').max('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rolling_pl = ds_pl[['gradh500','gradh900']].rolling(time=24).max().resample(time='1D').max('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruled-based algorithm\n",
    "\n",
    "# da = xr.where((ds_roll.prate<0.12) , 99, -1)\n",
    "\n",
    "# 1. 0.12 < Precipitation rate <= 0.5 and CAPE < 500\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate <= 0.5/3600, ds_rolling_sl.cape<500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_sl.prate >0.12/3600, a)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 40, -1)\n",
    "\n",
    "# 2. 0.12 < Precipitation rate <= 0.5 and CAPE >= 500\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate <= 0.5/3600, ds_rolling_sl.cape>=500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_sl.prate >0.12/3600, a)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 40, da)\n",
    "\n",
    "# 3. Precipitation rate > 0.5, CAPE < 500, 500mb Pressure gradient <=9.5, 900 mb Pressure gradient < 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape<500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500<=9.5 ,ds_rolling_pl.gradh900<8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 30, da)\n",
    "\n",
    "# 4. Precipitation rate > 0.5, CAPE >= 500, 500mb Pressure gradient <=9.5, 900 mb Pressure gradient < 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape>=500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500<=9.5 ,ds_rolling_pl.gradh900<8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 33, da)\n",
    "\n",
    "# 5. Precipitation rate > 0.5, CAPE < 500, 500mb Pressure gradient >9.5, 900 mb Pressure gradient >= 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape<500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500>9.5 ,ds_rolling_pl.gradh900>=8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 10, da)\n",
    "\n",
    "# 6. Precipitation rate > 0.5, CAPE >= 500, 500mb Pressure gradient >9.5, 900 mb Pressure gradient >= 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape>=500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500>9.5 ,ds_rolling_pl.gradh900>=8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 13, da)\n",
    "\n",
    "# 7. Precipitation rate > 0.5, CAPE < 500, 500mb Pressure gradient <=9.5, 900 mb Pressure gradient >= 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape<500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500<=9.5 ,ds_rolling_pl.gradh900>=8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 60, da)\n",
    "\n",
    "# 8. Precipitation rate > 0.5, CAPE >= 500, 500mb Pressure gradient <=9.5, 900 mb Pressure gradient >= 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape>=500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500<=9.5 ,ds_rolling_pl.gradh900>=8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 63, da)\n",
    "\n",
    "# 9. Precipitation rate > 0.5, CAPE < 500, 500mb Pressure gradient >9.5, 900 mb Pressure gradient < 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape<500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500>9.5 ,ds_rolling_pl.gradh900<8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 60, da)\n",
    "\n",
    "# 10. Precipitation rate > 0.5, CAPE >= 500, 500mb Pressure gradient >9.5, 900 mb Pressure gradient < 8\n",
    "a = xr.ufuncs.logical_and(ds_rolling_sl.prate>0.5/3600 ,ds_rolling_sl.cape>=500)\n",
    "b = xr.ufuncs.logical_and(ds_rolling_pl.gradh500>9.5 ,ds_rolling_pl.gradh900<8)\n",
    "c = xr.ufuncs.logical_and(a,b)\n",
    "da = xr.where(c, 63, da)\n",
    "\n",
    "# 10. Precipitation rate <0.12\n",
    "da = xr.where(ds_rolling_sl.prate<=0.12/3600, 99, da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'prelim_type'\n",
    "\n",
    "da_values = da.sel(time='1979-09-15')\n",
    "\n",
    "df_da = da_values.to_dataframe(name=variable).reset_index()\n",
    "\n",
    "labels = hv.Labels({('x', 'y'): df_da[['longitude','latitude']],\n",
    "                    'text': [i for i in df_da[variable]]},\n",
    "                                           ['x', 'y'], 'text') \n",
    "\n",
    "da_values.hvplot(x='longitude', y='latitude', cmap='isolum',\n",
    "                title='1979-09-15 : Preliminary storm type')*labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Automated storm typing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM THIS POINT ON, THIS IS A WORK IN PROGRESS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
